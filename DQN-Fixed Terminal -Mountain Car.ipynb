{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline \n",
    "\n",
    "1. DQN network which is part of the table and strategy used by the agent\n",
    "\n",
    "2. Epsilong greedy strategy also used by the agent. \n",
    "\n",
    "I changed so that the q network actually recognizes there is no reward for being done, optimizing the performance. \n",
    "\n",
    "Basically previous I arbitrarily said R at last i was a very negative value. While that's fine, it's important that the max q prediction is also 0. Now I do masking to do this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 1e-3\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input the shape of observations, which is 4 for cartpole.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=obs_shape, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32,out_features=24)\n",
    "        self.out = nn.Linear(in_features=24, out_features=3)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # convert current state to a tensor\n",
    "                state = torch.tensor(state).reshape(-1,2).float()\n",
    "                return policy_net(state).argmax().to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(obs_shape=2).to(device)\n",
    "target_net = DQN(obs_shape=2).to(device) #this one just gets updated occasionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_net(torch.randn(2,4)) #confirm this works, output is 2 actions of 2 random obs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replay memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.tensor(batch.state)\n",
    "    t2 = torch.tensor(batch.action)\n",
    "    t3 = torch.tensor(batch.reward)\n",
    "    t4 = torch.tensor(batch.next_state)\n",
    "    t5 = torch.tensor(batch.done)\n",
    "#should I include a done bool?\n",
    "    return (t1.float(),t2.float(),t3.float(),t4.float(),t5.float())\n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward','done')\n",
    ")\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1).long())\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states,dones):  #if done, need to include to set as 0   \n",
    "        done_mask = dones == 1 # if true, set to 0\n",
    "        values = target_net(next_states).max(dim=1)[0].detach()\n",
    "        values[done_mask] = 0.\n",
    "        return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, env.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "RENDER_TIME = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "RENDER = True\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() #initial state\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        timestep += 1\n",
    "        action = agent.select_action(state, policy_net).item()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(Experience(state, action, next_state, reward,done))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        #once enough replays are collected so it is possible to learn. \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = extract_tensors(experiences)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states,dones)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    episode_durations.append(timestep)\n",
    "    if episode % RENDER_TIME == 0:\n",
    "        RENDER = True\n",
    "    else: RENDER = False\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
