{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n",
    "\n",
    "\n",
    "In this notebook, I work through OpenAI's Gym, and the cartpole problem in particular. Applying a Deep Q Network, it is possible to \"win\" this game after a blend of exploration and exploitation of this space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll do my best to document what each piece of the code does, and provide sources for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies \n",
    "\n",
    "#Python stuff \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "#DL Stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#RL Stuff\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "#visualization stuff \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First define the Deep Q Network\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Neural Network to approximate the Q function. \n",
    "    \n",
    "    \n",
    "    https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-q-learning-and-linear-function-approximation/\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=obs_shape, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32,out_features=24)\n",
    "        self.out = nn.Linear(in_features=24, out_features=2)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    \"\"\"\n",
    "    Strategy for deciding between exploring and exploiting Q table.\n",
    "\n",
    "    https://jamesmccaffrey.wordpress.com/2017/11/30/the-epsilon-greedy-algorithm/\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    The class for the agent which interacts with the cartpole problem. \n",
    "    \n",
    "    Tracks the current step, it's action space, and decides whether or not to \n",
    "    explore or exploit based on rate vs random.random()\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # convert current state to a tensor\n",
    "                state = torch.tensor(state).reshape(-1,4).float()\n",
    "                return policy_net(state).argmax().to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for trajectories. \n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward','done')\n",
    ")\n",
    "\n",
    "#experience is stored in the main loop and contains state action pairs, with corresponding results\n",
    "\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    # Used as part of experience replay and calculating Q values of each at every \n",
    "    # iteration. \n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.tensor(batch.state)\n",
    "    t2 = torch.tensor(batch.action)\n",
    "    t3 = torch.tensor(batch.reward)\n",
    "    t4 = torch.tensor(batch.next_state)\n",
    "    t5 = torch.tensor(batch.done)\n",
    "    return (t1.float(),t2.float(),t3.float(),t4.float(),t5.float())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    A tensor of defined capacity containing a given number of experiences.\n",
    "    \n",
    "    \n",
    "    Sampled randomly to promote unrelated observations, \n",
    "    and fit DQN with those.\n",
    "    \n",
    "    Creating a certain capacity pushes out oldest experiences as time goes on, and \n",
    "    if coupling this with epsilon greedy strategy, means newer experiences are typically\n",
    "    exploits. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    \"\"\"\n",
    "    Method to calculate Q values. \n",
    "    \n",
    "    More specifically, calculates the Q value for the given states, \n",
    "\n",
    "    and then the max Q value of the next state, provided the episode is not done.\n",
    "    \n",
    "    If done is True or 1, then the next Q value is just 0. \n",
    "    \n",
    "    \n",
    "    These two values are important for value iteration. Get Current is the predicted Q\n",
    "    \n",
    "    and get_next is related to the Bellman Optimality of the true Q value. \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1).long())\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states,dones):  #if done, need to include to set as 0   \n",
    "        done_mask = dones == 1 # if true, set to 0\n",
    "        values = target_net(next_states).max(dim=1)[0].detach()\n",
    "        values[done_mask] = 0.\n",
    "        return values\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything has been initially defined, we can begin to populate this notebook with the task at hand, along with tuneable parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuneable parameters\n",
    "\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 1e-3\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Policy net is what is optimized, \n",
    "# and target net is an occasionally updated network used for finding Q*.\n",
    "\n",
    "policy_net = DQN(obs_shape=4).to(device)\n",
    "target_net = DQN(obs_shape=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize more objects. \n",
    "\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy, env.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "RENDER_TIME = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop:\n",
    "\n",
    "Iterate over episodes, iterate over timesteps in episode, save that experience, train the DQN, and all the while take steps according to epsilon greedy strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "RENDER = True\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() #initial state\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render(mode = 'human')\n",
    "        timestep += 1\n",
    "        action = agent.select_action(state, policy_net).item()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.push(Experience(state, action, next_state, reward,done))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        #once enough replays are collected so it is possible to learn. \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = extract_tensors(experiences)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states,dones)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    episode_durations.append(timestep)\n",
    "    if episode % RENDER_TIME == 0:\n",
    "        RENDER = True\n",
    "    else: RENDER = False\n",
    "    \n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
