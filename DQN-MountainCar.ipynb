{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Outline \n",
    "\n",
    "1. DQN network which is part of the table and strategy used by the agent\n",
    "\n",
    "2. Epsilong greedy strategy also used by the agent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 1e-4\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = len(env.reset())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input the shape of observations, which is 4 for cartpole.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_shape):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.fc1 = nn.Linear(in_features=obs_shape, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.fc3 = nn.Linear(in_features=32,out_features=24)\n",
    "        self.out = nn.Linear(in_features=24, out_features=3)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "\n",
    "        t = self.out(t)\n",
    "        return t\n",
    "    \n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # convert current state to a tensor\n",
    "                state = torch.tensor(state).reshape(-1,2).float()\n",
    "                return policy_net(state).argmax().to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(obs_shape=obs_shape).to(device)\n",
    "target_net = DQN(obs_shape=obs_shape).to(device) #this one just gets updated occasionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_net(torch.randn(2,4)) #confirm this works, output is 2 actions of 2 random obs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now replay memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.tensor(batch.state)\n",
    "    t2 = torch.tensor(batch.action)\n",
    "    t3 = torch.tensor(batch.reward)\n",
    "    t4 = torch.tensor(batch.next_state)\n",
    "\n",
    "    return (t1.float(),t2.float(),t3.float(),t4.float())\n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1).long())\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        return target_net(next_states).max(dim=1)[0].detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent = Agent(strategy, env.action_space.n, device)\n",
    "memory = ReplayMemory(memory_size)\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "RENDER_TIME = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "RENDER = False\n",
    "cumulative_rewards = [] \n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset() #initial state\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        timestep += 1\n",
    "        action = agent.select_action(state, policy_net).item()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if reward != -1 :\n",
    "            \n",
    "            print('Won in episode', episode, ' in ', episode_durations)\n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "\n",
    "        #once enough replays are collected so it is possible to learn. \n",
    "        if memory.can_provide_sample(batch_size):\n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    episode_durations.append(timestep)\n",
    "    if episode % RENDER_TIME == 0:\n",
    "        RENDER = True\n",
    "    else: RENDER = False\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d66f8d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I owe the utmost gratitude to deeplizard, Sentdex, and tds for getting me started on this, but here is how you create a pytorch dqn using the default observation values !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.sample(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
